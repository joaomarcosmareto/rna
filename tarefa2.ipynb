{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definindo imports\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronJM(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, num_epocas = 10000, taxa_aprendizado = 0.01):\n",
    "        self.num_epocas = num_epocas\n",
    "        self.taxa_aprendizado = taxa_aprendizado\n",
    "        self.bias = 0\n",
    "        self.pesos = None\n",
    "        self.erros = None\n",
    "        self.matriz_pesos = []\n",
    "        self.acuracia = []\n",
    "        \n",
    "    def funcao_treina(self, x):\n",
    "        \n",
    "        #insere o bias no vetor de atributos\n",
    "        x_bias = np.hstack((self.bias, x))\n",
    "        \n",
    "        #calcula o campo induzido\n",
    "        v = np.dot(self.pesos, x_bias)\n",
    "        \n",
    "        #calcula a saída do perceptron\n",
    "        y_aux = self.funcao_ativacao(v)\n",
    "        \n",
    "        return y_aux, x_bias\n",
    "\n",
    "    def funcao_ativacao(self, x):\n",
    "        return 1 if (np.dot(self.pesos, x) >= self.bias) else 0\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        #converte o X e Y para o formato do numpy\n",
    "        #isso garante funcionamento caso eles venham de dataframe\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        \n",
    "        #inicia o vetor de pesos\n",
    "        self.pesos = np.ones(X.shape[1])\n",
    "        \n",
    "        max_acuracia = 0\n",
    "\n",
    "        #para todas as epocas\n",
    "        for i in range(self.num_epocas):\n",
    "            k = 0\n",
    "            for x, y in zip(X, Y):\n",
    "                \n",
    "                y_pred = self.funcao_ativacao(x)\n",
    "                if y == 1 and y_pred == 0:\n",
    "                    self.pesos = self.pesos + self.taxa_aprendizado * x\n",
    "                    self.bias  = self.bias  - self.taxa_aprendizado * 1\n",
    "                elif y == 0 and y_pred == 1:\n",
    "                    self.pesos = self.pesos - self.taxa_aprendizado * x\n",
    "                    self.bias  = self.bias  + self.taxa_aprendizado * 1\n",
    "                \n",
    "            self.matriz_pesos.append(self.pesos)    \n",
    "            self.acuracia.append(accuracy_score(self.predict(X), Y))\n",
    "            if (self.acuracia[i] > max_acuracia):\n",
    "                max_acuracia = self.acuracia[i]\n",
    "                chkptw = self.pesos\n",
    "                chkptb = self.bias\n",
    "        #checkpoint (Save the weights and b value)\n",
    "        self.pesos = chkptw\n",
    "        self.bias  = chkptb\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X = np.array(X)        \n",
    "        result = []\n",
    "        \n",
    "        for x in X:\n",
    "            y_predict = self.funcao_ativacao(x)\n",
    "            result.append(y_predict)\n",
    "        \n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "names = [\"Percep. - JM\", \"Percep. Scikit\", \"Percep. - JM - 50k\", \"Percep. Scikit - 50k\",]\n",
    "\n",
    "classifiers = [\n",
    "    PerceptronJM(num_epocas = 10000),\n",
    "    Perceptron(max_iter = 10000, random_state = 42),\n",
    "    PerceptronJM(num_epocas = 50000),\n",
    "    Perceptron(max_iter = 50000, random_state = 42)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teste 1 - tudo default\n",
    "X, y = make_classification(random_state = 42)\n",
    "linearly_separable_teste_1 = (X, y)\n",
    "\n",
    "# teste 2 - copia do https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state = 42, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(42)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable_teste_2 = (X, y)\n",
    "\n",
    "# teste 3 - aumentando número de amostras\n",
    "X, y = make_classification(n_samples = 5000, random_state = 42)\n",
    "linearly_separable_teste_3 = (X, y)\n",
    "\n",
    "# teste 4 - aumentando número de amostras e features\n",
    "X, y = make_classification(n_samples = 5000, n_features = 30, random_state = 42)\n",
    "linearly_separable_teste_4 = (X, y)\n",
    "\n",
    "# teste 5 - aumentando número de amostras e features, mas diminuindo o class_sep\n",
    "X, y = make_classification(n_samples = 5000, n_features = 30, class_sep = 0.8, random_state = 42)\n",
    "linearly_separable_teste_5 = (X, y)\n",
    "\n",
    "datasets = [\n",
    "    linearly_separable_teste_1,\n",
    "    linearly_separable_teste_2,\n",
    "    linearly_separable_teste_3,\n",
    "    linearly_separable_teste_4,\n",
    "    linearly_separable_teste_5\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste: 1\n",
      "Classificador: Percep. - JM\n",
      "Score: 0.825\n",
      "\n",
      "Teste: 1\n",
      "Classificador: Percep. Scikit\n",
      "Score: 0.85\n",
      "\n",
      "Teste: 1\n",
      "Classificador: Percep. - JM - 50k\n",
      "Score: 0.825\n",
      "\n",
      "Teste: 1\n",
      "Classificador: Percep. Scikit - 50k\n",
      "Score: 0.85\n",
      "\n",
      "Teste: 2\n",
      "Classificador: Percep. - JM\n",
      "Score: 0.85\n",
      "\n",
      "Teste: 2\n",
      "Classificador: Percep. Scikit\n",
      "Score: 0.85\n",
      "\n",
      "Teste: 2\n",
      "Classificador: Percep. - JM - 50k\n",
      "Score: 0.85\n",
      "\n",
      "Teste: 2\n",
      "Classificador: Percep. Scikit - 50k\n",
      "Score: 0.85\n",
      "\n",
      "Teste: 3\n",
      "Classificador: Percep. - JM\n",
      "Score: 0.7465\n",
      "\n",
      "Teste: 3\n",
      "Classificador: Percep. Scikit\n",
      "Score: 0.8155\n",
      "\n",
      "Teste: 3\n",
      "Classificador: Percep. - JM - 50k\n",
      "Score: 0.7465\n",
      "\n",
      "Teste: 3\n",
      "Classificador: Percep. Scikit - 50k\n",
      "Score: 0.8155\n",
      "\n",
      "Teste: 4\n",
      "Classificador: Percep. - JM\n",
      "Score: 0.8175\n",
      "\n",
      "Teste: 4\n",
      "Classificador: Percep. Scikit\n",
      "Score: 0.791\n",
      "\n",
      "Teste: 4\n",
      "Classificador: Percep. - JM - 50k\n",
      "Score: 0.8175\n",
      "\n",
      "Teste: 4\n",
      "Classificador: Percep. Scikit - 50k\n",
      "Score: 0.791\n",
      "\n",
      "Teste: 5\n",
      "Classificador: Percep. - JM\n",
      "Score: 0.742\n",
      "\n",
      "Teste: 5\n",
      "Classificador: Percep. Scikit\n",
      "Score: 0.751\n",
      "\n",
      "Teste: 5\n",
      "Classificador: Percep. - JM - 50k\n",
      "Score: 0.742\n",
      "\n",
      "Teste: 5\n",
      "Classificador: Percep. Scikit - 50k\n",
      "Score: 0.751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        \n",
    "        print(\"Teste: {}\\nClassificador: {}\\nScore: {}\\n\".format(ds_cnt+1, name, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
